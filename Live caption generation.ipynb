{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading https://files.pythonhosted.org/packages/96/30/99bd865802cd5f425c42efd2ee4e10bd3bc605640008f03e3c72a1dbe320/opencv_python-4.0.0.21-cp36-cp36m-win_amd64.whl (30.4MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in d:\\anaconda\\envs\\tensorflow\\lib\\site-packages (from opencv-python) (1.15.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.0.0.21\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import CustomObjectScope\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from pickle import load\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def similarity(frame1, frame2):\n",
    "    # Transforme image to grayscale\n",
    "    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    hist1 = cv2.calcHist([gray1],[0],None,[256],[0,256])\n",
    "    hist2 = cv2.calcHist([gray2],[0],None,[256],[0,256])\n",
    "    comp = cv2.compareHist(hist1, hist2, 0)\n",
    "    return comp\n",
    "\n",
    "def extract_features(frame):\n",
    "\t# extract features from frame\n",
    "\tresized_image = cv2.resize(frame, (224, 224)) \n",
    "\timage = img_to_array(resized_image)\n",
    "\t# reshape data for the model\n",
    "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t# prepare the image for the VGG model\n",
    "\timage = preprocess_input(image)\n",
    "\t# get features\n",
    "\tfeatures = model_vgg16.predict(image, verbose=0)\n",
    "\treturn features\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    " \n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "\t# seed the generation process\n",
    "\tin_text = 'startseq'\n",
    "\t# iterate over the whole length of the sequence\n",
    "\tfor i in range(max_length):\n",
    "\t\t# integer encode input sequence\n",
    "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pad input\n",
    "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
    "\t\t# predict next word\n",
    "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
    "\t\t# convert probability to integer\n",
    "\t\tyhat = argmax(yhat)\n",
    "\t\t# map integer to word\n",
    "\t\tword = word_for_id(yhat, tokenizer)\n",
    "\t\t# stop if we cannot map the word\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\t# append as input for generating the next word\n",
    "\t\tin_text += ' ' + word\n",
    "\t\t# stop if we predict the end of the sequence\n",
    "\t\tif word == 'endseq':\n",
    "\t\t\tbreak\n",
    "\treturn in_text\n",
    "\n",
    "\n",
    "os.chdir('/Users/Adrien Delpierre/Documents/Projet')\n",
    "model_vgg16 = VGG16()\n",
    "# remove the classifier layers\n",
    "model_vgg16 = Model(inputs=model_vgg16.inputs, outputs=model_vgg16.layers[-2].output)\n",
    "\n",
    "with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
    "    model = load_model('model_19.h5')\n",
    "    \n",
    "    \n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 40\n",
    "\n",
    "# Text display\n",
    "font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "bottomSideOfText       = (200,350)\n",
    "topLeftCornerOfText    = (350, 10)\n",
    "fontScale              = 1\n",
    "fontColor              = (0,255,70)\n",
    "lineType               = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq endseq\n",
      "0.6623630672569446\n",
      "startseq endseq\n",
      "0.6843072688658745\n",
      "startseq a painted wood dog startseq a two shirt on pool while startseq over with a bar and a dog startseq a two shirt on a of shirt on a of shirt on a of shirt on a of shirt on\n",
      "0.6847647871777065\n",
      "startseq endseq\n",
      "0.6872869112317306\n",
      "startseq endseq\n",
      "0.6966511121405158\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,224)\n",
    "cap.set(4,224)\n",
    "frameRate = cap.get(5)\n",
    "_, prev_frame = cap.read()\n",
    "\n",
    "compteur = 1\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    _, frame = cap.read()\n",
    "    # Our operations on the frame come here\n",
    "    similar = similarity(frame, prev_frame)\n",
    "        \n",
    "    # Our operations on the frame come here\n",
    "    if similar < 0.7:\n",
    "        frame_features = extract_features(frame)\n",
    "        # generate description\n",
    "        description = generate_desc(model, tokenizer, frame_features, max_length)\n",
    "        print(description)\n",
    "        print(similar)\n",
    "                \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    if (compteur % 15 == 0):\n",
    "        prev_frame = frame\n",
    "    compteur += 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
