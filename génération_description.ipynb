{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "drWxAaMqmNJT"
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
    "# https://towardsdatascience.com/image-captioning-in-deep-learning-9cd23fb4d8d2\n",
    "# https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 97402.0,
     "status": "ok",
     "timestamp": 1.547205517015E12,
     "user": {
      "displayName": "Frédéric Lavner",
      "photoUrl": "https://lh3.googleusercontent.com/-yShGH4lZVBk/AAAAAAAAAAI/AAAAAAAAAHo/yBqG9iWm3L4/s64/photo.jpg",
      "userId": "08062033154804223441"
     },
     "user_tz": -60.0
    },
    "id": "qO14EJs6JOUm",
    "outputId": "31b9c14c-eb7a-40f0-c761-c187e609d19d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/Adrien Delpierre/Documents/Projet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in d:\\anaconda\\envs\\tensorflow\\lib\\site-packages (5.4.1)\n",
      "Collecting nltk\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\n",
      "Requirement already satisfied: six in d:\\anaconda\\envs\\tensorflow\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Collecting singledispatch (from nltk)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk: started\n",
      "  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Adrien Delpierre\\AppData\\Local\\pip\\Cache\\wheels\\4b\\c8\\24\\b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\n",
      "Successfully built nltk\n",
      "Installing collected packages: singledispatch, nltk\n",
      "Successfully installed nltk-3.4 singledispatch-3.4.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from PIL import *\n",
    "import numpy as np\n",
    "from pickle import dump, load\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_K5ZyakKj90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory):\n",
    "\tmodel = VGG16()\n",
    "\t# remove the classifier layers\n",
    "\tmodel.layers.pop()\n",
    "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "\tmodel.summary()\n",
    "\t# extract features from each photo\n",
    "\tfeatures = {}\n",
    "\tfor name in listdir(directory):\n",
    "\t\t# load an image from file\n",
    "\t\tfilename = directory + '/' + name\n",
    "\t\timage = load_img(filename, target_size=(224, 224))\n",
    "\t\t# convert the image pixels to a numpy array\n",
    "\t\timage = img_to_array(image)\n",
    "\t\t# reshape data for the model\n",
    "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t\t# prepare the image for the VGG model\n",
    "\t\timage = preprocess_input(image)\n",
    "\t\t# get features\n",
    "\t\tfeature = model.predict(image, verbose=0)\n",
    "\t\t# get image id\n",
    "\t\timage_id = name.split('.')[0]\n",
    "\t\t# store feature\n",
    "\t\tfeatures[image_id] = feature\n",
    "\t\t#print('>%s' % name)\n",
    "\treturn features\n",
    "\n",
    "features = extract_features('Flicker8k_Dataset')\n",
    "# Saving features in a file\n",
    "dump(features, open('features.pkl', 'wb'));\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1U5cQF2f1O3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8092 descriptions chargées \n",
      "Taille du vocabulaire : 9630 \n"
     ]
    }
   ],
   "source": [
    "# On associe à l'id de chaque photo ses descriptions dans le dictionnaire mapping.\n",
    "def load_desc(file_name):\n",
    "  mapping = {}\n",
    "  with open(file_name) as f:\n",
    "    data = f.readlines()\n",
    "    for line in data:\n",
    "      tokens = line.split()\n",
    "      if len(line) < 2:\n",
    "        continue\n",
    "      # take the first token as the image id, the rest as the description\n",
    "      image_id, image_desc = tokens[0], tokens[1:]\n",
    "      # remove filename from image id\n",
    "      image_id = image_id.split('.')[0]\n",
    "      # convert description tokens back to string\n",
    "      image_desc = ' '.join(image_desc)\n",
    "      # create the list if needed\n",
    "      if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "      # store description\n",
    "      mapping[image_id].append(image_desc)\n",
    "  return mapping\n",
    "\n",
    "\n",
    "def clean_desc(descriptions):\n",
    "  for desc_list in descriptions.values():\n",
    "    desc_list = [text_to_word_sequence(desc, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~)0123456789')\n",
    "                 for desc in desc_list]\n",
    "\n",
    "\n",
    "def to_vocabulary(descriptions):\n",
    "\t# build a list of all description strings\n",
    "\tall_desc = set()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "\n",
    "def save_desc(descriptions, filename):\n",
    "  with open(filename, 'w') as f:\n",
    "    lines = []\n",
    "    for key, desc_list in descriptions.items():\n",
    "      lines = lines + [key + ' ' + desc for desc in desc_list]\n",
    "    f.write('\\n'.join(lines))\n",
    "    \n",
    "\n",
    "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "# parse descriptions\n",
    "descriptions = load_desc(filename)\n",
    "print('%d descriptions chargées ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_desc(descriptions)\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Taille du vocabulaire : %d ' % len(vocabulary))\n",
    "# save to file\n",
    "save_desc(descriptions, 'Flickr8k_text/descriptions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8rY3gxbM5EE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : 6000\n",
      "Descriptions : train = 6000\n",
      "Photos : train = 6000\n",
      "Taille du vocabulaire : 8494\n"
     ]
    }
   ],
   "source": [
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "  with open(filename) as f:\n",
    "    dataset = [line.split('.')[0] for line in f.readlines() if len(line) >= 1]\n",
    "  return set(dataset)\n",
    "\n",
    "def load_clean_desc(filename, dataset):\n",
    "  with open(filename) as f:\n",
    "    descriptions = {}\n",
    "    for line in f.readlines():\n",
    "      tokens = line.split()\n",
    "      # split id from description\n",
    "      image_id, image_desc = tokens[0], tokens[1:]\n",
    "      # skip images not in the set\n",
    "      if image_id in dataset:\n",
    "        if image_id not in descriptions:\n",
    "          descriptions[image_id] = []\n",
    "        # wrap description in tokens\n",
    "        desc = 'seqstart ' + ' '.join(image_desc) + ' seqend'\n",
    "        descriptions[image_id].append(desc)\n",
    "  return descriptions\n",
    "\n",
    "\n",
    "def load_photo_features(filename, dataset):\n",
    "\t# load all features\n",
    "  with open(filename, 'rb') as f:\n",
    "    all_features = load(f)\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "  return features\n",
    "\n",
    "\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset : %d' % len(train))\n",
    "train_descriptions = load_clean_desc('Flickr8k_text/descriptions.txt', train)\n",
    "print('Descriptions : train = %d' % len(train_descriptions))\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos : train = %d' % len(train_features))\n",
    "\n",
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(desc):\n",
    "\ttext = to_lines(descriptions)\n",
    "\ttok = Tokenizer()\n",
    "\ttok.fit_on_texts(text)\n",
    "\treturn tok\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Taille du vocabulaire : %d' % vocab_size)\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJgxNbnKefVw"
   },
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "\tX1, X2, y = [], [], []\n",
    "\t# walk through each description for the image\n",
    "\tfor desc in desc_list:\n",
    "\t\t# encode the sequence\n",
    "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t# split one sequence into multiple X,y pairs\n",
    "\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t# split into input and output pair\n",
    "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t# pad input sequence\n",
    "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t# encode output sequence\n",
    "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t# store\n",
    "\t\t\tX1.append(photo)\n",
    "\t\t\tX2.append(in_seq)\n",
    "\t\t\ty.append(out_seq)\n",
    "\treturn np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOahlbi4Rcju"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "\t# feature extractor model\n",
    "\tinputs1 = Input(shape=(1000,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu')(fe1)\n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_l,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(256)(se2)\n",
    "\t# decoder model\n",
    "\tdecoder1 = add([fe2, se3])\n",
    "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tmodel.summary()\n",
    "\t#plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length):\n",
    "\t# loop for ever over images\n",
    "\twhile 1:\n",
    "\t\tfor key, desc_list in descriptions.items():\n",
    "\t\t\t# retrieve the photo feature\n",
    "\t\t\tphoto = photos[key][0]\n",
    "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
    "\t\t\tyield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Id_7A7gPclWS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 8494\n",
      "Description Length: 40\n"
     ]
    }
   ],
   "source": [
    "# train dataset\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_desc('Flickr8k_text/descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_l = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 40, 256)      2174464     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1000)         0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 40, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          256256      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          525312      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense_4[0][0]                    \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 8494)         2182958     dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,204,782\n",
      "Trainable params: 5,204,782\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 729s 122ms/step - loss: 4.6069\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 722s 120ms/step - loss: 3.7442\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 722s 120ms/step - loss: 3.4409\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 722s 120ms/step - loss: 3.2507\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 726s 121ms/step - loss: 3.1163\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 726s 121ms/step - loss: 3.0141\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 724s 121ms/step - loss: 2.9340\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 724s 121ms/step - loss: 2.8666\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 746s 124ms/step - loss: 2.8115\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 741s 124ms/step - loss: 2.7629\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "\t# create the data generator\n",
    "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_l)\n",
    "\t# fit for one epoch\n",
    "\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\t# save model\n",
    "\tmodel.save('model_' + str(i) + '.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "génération_description.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
